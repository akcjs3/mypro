# analyzer.py
import os
import json
import csv
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple

import numpy as np
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import load_img, img_to_array

from fuzzy_system import apply_fuzzy_rules


# =======================================================
# âœ… ê²½ë¡œ ì„¤ì • (backend ê¸°ì¤€ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ)
# - ê¸°ì¡´ ì½”ë“œê°€ "../data/..." ë¡œ ì¡í˜€ì„œ backend ë°–ì„ ë´ë²„ë¦¼
# - í˜„ì¬ ë¡œê±°ë“¤ì´ backend/data/* ì— ì €ì¥í•˜ë¯€ë¡œ ê·¸ìª½ì„ 1ìˆœìœ„ë¡œ ì‚¬ìš©
# =======================================================
BASE_DIR = Path(__file__).resolve().parent

# backend/data
DATA_DIR = (BASE_DIR / "data").resolve()
DATA_DIR.mkdir(parents=True, exist_ok=True)

# í˜¹ì‹œ ì˜ˆì „ êµ¬ì¡°(backend/../data)ë„ ë‚¨ì•„ìˆë‹¤ë©´ ë³´ì¡°ë¡œ íƒìƒ‰
LEGACY_DATA_DIR = (BASE_DIR / ".." / "data").resolve()

SESSION_LOG_DIR = DATA_DIR / "session_logs"
SESSION_LOG_DIR.mkdir(parents=True, exist_ok=True)

SCREENS_DIR = DATA_DIR / "screens"
SCREENS_DIR.mkdir(parents=True, exist_ok=True)

WINDOW_CSV = DATA_DIR / "window_log.csv"
INPUT_CSV = DATA_DIR / "input_log.csv"
PROCESS_CSV = DATA_DIR / "process_log.csv"
SCREEN_CSV = DATA_DIR / "screen_log.csv"

ANALYZER_BASE_DIR = Path(__file__).resolve().parent
ANALYZER_DATA_DIR = (ANALYZER_BASE_DIR / "data").resolve()
ANALYZER_SESSION_LOG_DIR = ANALYZER_DATA_DIR / "session_logs"
ANALYZER_SESSION_LOG_DIR.mkdir(parents=True, exist_ok=True)


# -------------------------------------------------------
# ğŸ”¥ 1. TITLE KEYWORDS (ëª¨ë“  ë¼ë²¨ í‚¤ì›Œë“œ) â€” ì ˆëŒ€ ì‚­ì œ/ìˆ˜ì • ì•ˆ í•¨
# -------------------------------------------------------
TITLE_KEYWORDS = {
    "game": ["game", "steam", "league of legends", "lol", "valorant", "overwatch", "maplestory", "lostark", "í…ŒíŠ¸ë¦¬ìŠ¤", "ê²Œì„", "ì¹¼"],
    "study": ["ppt", "pdf", "study", "homework", "report", "notion", "stackoverflow", "postech", "lecture", "visual studio code", "vscode", "code.exe",
        "pycharm", "intellij", "android studio",
        "jupyter", "colab", "terminal", "cmd", "powershell","inflearn", "ì¸í”„ëŸ°", "ê°•ì˜", "í•™ìŠµ í˜ì´ì§€", "lecture video", "ì¤‘ë¶€ëŒ€í•™êµ", "lms", "ê°•ì¢Œ"],
    "webtoon": ["webtoon", "naver webtoon", "kakao webtoon", "toon","ë§Œí™”", "ì›¹íˆ°", "ë‰´í† ë¼"],
    "sns": ["instagram", "insta", "facebook", "twitter", "tiktok", "reels", "shorts", "ì¸ìŠ¤íƒ€", "ì¹´ì¹´ì˜¤í†¡"],
    "youtube-ent": ["youtube", "yt", "tv", "netflix", "tving", "watching", "drama"],
    "youtube-music": ["music", "song", "lyrics", "audio", "mv", "playlist", "melody", "ë®¤ì§", "ê°€ì‚¬"]
}

# -------------------------------------------------------
# ğŸ”¥ 2. MUSIC KEYWORDS â€” ì ˆëŒ€ ì¤„ì´ì§€ ì•ŠìŒ
# -------------------------------------------------------
MUSIC_KEYWORDS = [
    "music", "song", "lyrics", "lyric", "audio", "mv", "playlist", "melody",
    "ê°€ì‚¬", "ë…¸ë˜", "ë®¤ì§", "audio only", "official audio"
]

# -------------------------------------------------------
# ğŸ”¥ ìŒì•… ì œëª© íŒë³„ í•¨ìˆ˜
# -------------------------------------------------------
def is_music_title(title: str) -> bool:
    if not title:
        return False
    title_lower = title.lower()
    for kw in MUSIC_KEYWORDS:
        if kw.lower() in title_lower:
            return True
    return False

# -------------------------------------------------------
# ğŸ”¥ TITLE ê¸°ë°˜ ë¼ë²¨
# -------------------------------------------------------
def apply_keyword_priority(window_titles: list) -> str | None:
    if not window_titles:
        return None

    title_str = " ".join(t.lower() for t in window_titles if isinstance(t, str))

    for label, keywords in TITLE_KEYWORDS.items():
        for kw in keywords:
            if kw.lower() in title_str:
                return label

    return None


# -------------------------------------------------------
# ğŸ”¥ ë‹¨ì¼ ì œëª© -> í‚¤ì›Œë“œ ë¼ë²¨ ì¶”ì • (fallbackìš©)
# -------------------------------------------------------
def label_from_title(title: str) -> str:
    if not title:
        return "other"
    tl = title.lower()
    for label, keywords in TITLE_KEYWORDS.items():
        for kw in keywords:
            if kw.lower() in tl:
                return label
    return "other"


# -------------------------------------------------------
# ğŸ”¥ Sequence-based ë¹„ìœ¨ ê³„ì‚°
# -------------------------------------------------------
def compute_sequence_distribution(window_labels: list, session_sec: float):
    if not window_labels:
        return {}

    per = {}
    total = len(window_labels)
    for w in window_labels:
        per[w] = per.get(w, 0) + 1

    return {k: (v / total) * 100 for k, v in per.items()}


# =======================================================
# âœ… CNN ëª¨ë¸ ë¡œë“œ (ê²½ë¡œ ì—¬ëŸ¬ ê°œ ì‹œë„)
# =======================================================
LABELS = ["game", "other", "sns", "study", "webtoon", "youtube-ent", "youtube-music"]

def _resolve_cnn_model_path() -> Optional[str]:
    candidates = [
        str(BASE_DIR / "monitor_model.h5"),          # backend/monitor_model.h5
        str(BASE_DIR / "models" / "cnn_model.h5"),   # backend/models/cnn_model.h5 (ì˜ˆì „)
        str(LEGACY_DATA_DIR / "models" / "cnn_model.h5"),
    ]
    for p in candidates:
        if os.path.exists(p):
            return p
    return None

CNN_MODEL_PATH = _resolve_cnn_model_path()
cnn_model = None
if CNN_MODEL_PATH:
    try:
        cnn_model = load_model(CNN_MODEL_PATH)
        print(f"[Analyzer] CNN ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {CNN_MODEL_PATH}")
    except Exception as e:
        print("[Analyzer] CNN ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨:", e)
        cnn_model = None
else:
    print("âš  CNN ëª¨ë¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


# -------------------------------------------------------
# ğŸ”¥ SCREEN í´ë”ì—ì„œ ì´ë¯¸ì§€ ìˆ˜ì§‘ + CNN ì˜ˆì¸¡
# -------------------------------------------------------
def analyze_screen_images(session_id: str): # -> Tuple[Dict[str, float], Optional[str]]:
    """
    í•´ë‹¹ session_idì— í•´ë‹¹í•˜ëŠ” ìŠ¤ìƒ·ë“¤ë§Œ ëª¨ì•„ì„œ CNN ì˜ˆì¸¡.
    - screen_capture.py íŒŒì¼ëª…ì— session_id í¬í•¨ë¨.
    """
    if cnn_model is None:
        return ({label: 0.0 for label in LABELS}, None)

    folder = str(SCREENS_DIR)
    if not os.path.isdir(folder):
        return ({label: 0.0 for label in LABELS}, None)

    # ì„¸ì…˜ id í¬í•¨ëœ ìŠ¤í¬ë¦°ìƒ·ë§Œ
    files = [
        f for f in os.listdir(folder)
        if session_id in f and f.lower().endswith((".png", ".jpg", ".jpeg"))
    ]
    if not files:
     return ({label: 0.0 for label in LABELS}, None, 0)

    files.sort()
    recent_files = files[-8:]
 
    xs = []
    for fn in recent_files:
        p = os.path.join(folder, fn)
        try:
            img = load_img(p, target_size=(128, 128))
            arr = img_to_array(img)
            arr = np.expand_dims(arr, axis=0) / 255.0
            xs.append(arr)
        except Exception:
            continue

    if not xs:
        return ({label: 0.0 for label in LABELS}, None, len(recent_files))

    X = np.vstack(xs)
    logits = cnn_model.predict(X, verbose=0)
    probs = logits.mean(axis=0)
    probs = probs / (probs.sum() + 1e-8)

    probs_dict = {label: float(v) for label, v in zip(LABELS, probs)}
    top_label = LABELS[int(np.argmax(probs))]

    return probs_dict, top_label, len(recent_files)


# =======================================================
# âœ… CSV ë¡œë¶€í„° ì„¸ì…˜ ë¡œê·¸ ì¬êµ¬ì„± (json ì—†ì„ ë•Œ fallback)
# =======================================================
def _read_csv_rows(path: Path) -> List[Dict[str, str]]:
    if not path.exists():
        return []
    with path.open("r", newline="", encoding="utf-8") as f:
        return list(csv.DictReader(f))

def _filter_by_session(rows: List[Dict[str, str]], session_id: str) -> List[Dict[str, str]]:
    return [r for r in rows if str(r.get("session_id")) == str(session_id)]

def _parse_datetime_safe(s: str) -> Optional[datetime]:
    if not s:
        return None
    try:
        return datetime.fromisoformat(s)
    except Exception:
        return None

def build_logs_from_csv(session_id: str) -> Dict:
    """
    window_log.csv / input_log.csv / process_log.csv / screen_log.csvì—ì„œ
    í•´ë‹¹ session_idë§Œ ê³¨ë¼ logs êµ¬ì¡° ìƒì„±.
    """
    logs = {
        "session_id": session_id,
        "window": [],
        "input": [],
        "process": [],
        "screen": [],
        "idle_sec": 0,
        "session_start": None,
        "session_end": None,
    }

    # window
    w_rows = _filter_by_session(_read_csv_rows(WINDOW_CSV), session_id)
    for r in w_rows:
        logs["window"].append({
            "timestamp": r.get("timestamp"),
            "title": r.get("window_title") or "",
            "process_name": r.get("process_name") or "",
            "exe_path": r.get("exe_path") or "",
            "label": label_from_title(r.get("window_title") or ""),
        })

    # input
    i_rows = _filter_by_session(_read_csv_rows(INPUT_CSV), session_id)
    for r in i_rows:
        logs["input"].append({
            "timestamp": r.get("timestamp"),
            "event_type": r.get("event_type"),
            "key": r.get("key"),
            "button": r.get("button"),
        })

    # process
    p_rows = _filter_by_session(_read_csv_rows(PROCESS_CSV), session_id)
    for r in p_rows:
        logs["process"].append(r)

    # screen
    s_rows = _filter_by_session(_read_csv_rows(SCREEN_CSV), session_id)
    for r in s_rows:
        logs["screen"].append(r)

    # session start/end ì¶”ì •
    all_ts = []
    for r in (w_rows + i_rows + s_rows + p_rows):
        dt = _parse_datetime_safe(r.get("timestamp", ""))
        if dt:
            all_ts.append(dt)

    if all_ts:
        logs["session_start"] = min(all_ts).isoformat()
        logs["session_end"] = max(all_ts).isoformat()

    return logs


# =======================================================
# âœ… ë©”ì¸ ë¶„ì„ í•¨ìˆ˜
# =======================================================
MIN_SESSION_SEC = 30  # ì„œë²„ì™€ ë§ì¶°ë‘ 

def analyze_and_save(
    session_id: str,
    user_id: Optional[int] = None,
    selected_task: Optional[str] = None,
    usage_index: Optional[int] = None,
    **kwargs
):
    """
    server.pyì—ì„œ í˜¸ì¶œ:
      analyze_and_save(session_id=..., user_id=..., selected_task=..., usage_index=...)

    - ê¸°ì¡´ ê¸°ëŠ¥ ìµœëŒ€í•œ ë³´ì¡´ + csv fallback + í”„ë¡ íŠ¸ í•„ë“œ ë³´ê°•
    """

    # ---------------------------------------------------
    # 1) ì„¸ì…˜ ë¡œê·¸ json ë¡œë“œ ì‹œë„ (ì—†ìœ¼ë©´ csv fallback)
    # ---------------------------------------------------
    json_path = SESSION_LOG_DIR / f"{session_id}.json"
    logs = None
    if json_path.exists():
        try:
            with json_path.open("r", encoding="utf-8") as f:
                logs = json.load(f)
        except Exception:
            logs = None

    if logs is None:
        logs = build_logs_from_csv(session_id)

    # ---------------------------------------------------
    # 2) ì„¸ì…˜ ì‹œê°„ ê³„ì‚°
    # ---------------------------------------------------
    start_dt = _parse_datetime_safe(logs.get("session_start") or "")
    end_dt = _parse_datetime_safe(logs.get("session_end") or "")

    if start_dt and end_dt:
        session_sec = max((end_dt - start_dt).total_seconds(), 1.0)
    else:
        # ë¡œê·¸ê°€ ê±°ì˜ ì—†ì„ ë•Œ ìµœì†Œê°’
        session_sec = 1.0

    # ---------------------------------------------------
    # 3) window titles/labels
    # ---------------------------------------------------
    
    window_logs = logs.get("window", [])
    
    filtered_logs = []
    for w in window_logs:
        title = (w.get("title") or "").strip()
        tl = title.lower()
        if title == "":
            continue
        
        if "ì‘ì—… ì „í™˜" in title or "task switching" in tl:
            continue
        if "monitor sketcher" in tl:
            continue
        filtered_logs.append(w)  
        
        window_logs = filtered_logs
    window_titles = [w.get("title", "") for w in window_logs]
    window_labels = [w.get("label", "other") for w in window_logs]

    # keyword ê¸°ë°˜ ìš°ì„  ë¼ë²¨
    keyword_label = apply_keyword_priority(window_titles)

    # ---------------------------------------------------
    # 4) input count
    # ---------------------------------------------------
    input_logs = logs.get("input", [])
    key_count = 0
    mouse_count = 0
    for r in input_logs:
        et = (r.get("event_type") or "").lower()
        if et == "key_press":
            key_count += 1
        elif et in ("mouse_down", "mouse_up", "mouse_scroll"):
            mouse_count += 1

    # âœ… (ê¸°ì¡´ì— ì“°ë˜ input_ratio ëˆ„ë½ ë²„ê·¸ ìˆ˜ì •)
    total_input = key_count + mouse_count
    input_ratio = key_count / total_input if total_input > 0 else 0.0

    # ---------------------------------------------------
    # 5) sequence distribution
    # ---------------------------------------------------
    seq_percent = compute_sequence_distribution(window_labels, session_sec)
    seq_ratio = {k: v / 100.0 for k, v in seq_percent.items()}

    # ---------------------------------------------------
    # 6) screen(CNN) ë¶„ì„
    # ---------------------------------------------------
    screen_probs, screen_top, capture_count = analyze_screen_images(session_id)

    # ---------------------------------------------------
    # 7) fuzzy rules
    # ---------------------------------------------------
    last_window_label = window_labels[-1] if window_labels else None

    # ---- í¼ì§€ ì‹œìŠ¤í…œ ì ìš©(âœ… fuzzy_system.py ì‹œê·¸ë‹ˆì²˜ì— ë§ê²Œ dict 1ê°œë§Œ ì „ë‹¬) ---- #
    fuzzy_input = {
        "input": {
            "key_count": key_count,
            "mouse_count": mouse_count,
            "session_duration_sec": session_sec
        },
        "window": {
            # fuzzy_systemì€ top_label, titleì„ ë´„
            "top_label": last_window_label,
            "title": window_titles[-1] if window_titles else "",
            "window_labels": window_labels
        },
        "screen": {
            # fuzzy_systemì€ screen_top / screen_probsë¥¼ ë´„
            "screen_top": screen_top,
            "screen_probs": screen_probs
        },
        "selected_task": selected_task,
    }

    final_label = apply_fuzzy_rules(fuzzy_input)

    # ---------------------------------------------------
    # 8) idle / focus ê³„ì‚°
    # ---------------------------------------------------
    idle_sec = logs.get("idle_sec", 0) or 0
    idle_ratio = idle_sec / session_sec if session_sec > 0 else 0.0

    focus_ratio = seq_ratio.get(selected_task, 0.0)
    focus_percent = seq_percent.get(selected_task, 0.0)

    # ---------------------------------------------------
    # 9) ê²°ê³¼ êµ¬ì„± (í”„ë¡ íŠ¸ì—ì„œ ì“°ëŠ” í•„ë“œ í¬í•¨)
    # ---------------------------------------------------
    result = {
        "session_id": session_id,
        "user_id": user_id,
        "usage_index": usage_index,
        "selected_task": selected_task,
        "session_start": logs.get("session_start"),
        "session_end": logs.get("session_end"),

        "final_label": final_label,   # âœ… test.htmlì—ì„œ í•„ìš”
        "predicted": final_label,     # server.pyì—ì„œ ì“°ë˜ ì´ë¦„ ìœ ì§€

        # âœ… ì›ê·¸ë˜í”„ìš©
        "activity_distribution": {
            "percent": seq_percent,
            "ratio": seq_ratio,
        },

        # âœ… window ìƒì„¸
        "window": {
            "window_titles": window_titles,
            "window_labels": window_labels,
            "distribution": {
                "percent": seq_percent,
                "ratio": seq_ratio,
            },
        },

        # âœ… screen ìƒì„¸
        "screen": {
            "screen_probs": screen_probs,
            "screen_top": screen_top,
            "capture_count": capture_count,   # âœ… í”„ë¡ íŠ¸ìš©
            "num_captures": capture_count,    # âœ… alias
            "total_captures": capture_count,  # âœ… alias

        },

        # âœ… ì…ë ¥/ì°¸ì—¬ë„
        "input": {
            "key_count": key_count,
            "mouse_count": mouse_count,
        },

        "engagement": {
            "idle_percent": idle_ratio * 100,
            "idle_ratio": idle_ratio,
            "idle_time_sec": idle_sec,
            "input_per_min": total_input / max(session_sec / 60.0, 1.0),
            "session_duration_sec": session_sec,
            "total_input": total_input,
        },

        # âœ… process count (ìˆìœ¼ë©´)
        "process": {
            "process_count": len(logs.get("process", []))
        },

        # âœ… í™”ë©´/ì„œë²„ê°€ ì“°ë˜ í•„ë“œ ì¼ë¶€ë„ ìœ ì§€
        "focus_ratio": focus_ratio,
        "focus_percent": focus_percent,
        "inputPerMin": total_input / max(session_sec / 60.0, 1.0),
    }

    # ---------------------------------------------------
    # 10) ì €ì¥
    # ---------------------------------------------------
    out_path = SESSION_LOG_DIR / f"{session_id}_analysis.json"
    try:
        with out_path.open("w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=4)
    except Exception as e:
        print("[Analyzer] ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨:", e)

    return result

